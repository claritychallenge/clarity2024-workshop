---
layout: page
title: Programme
---

Timings and session details are provided below. All times are UK local time (i.e., <b>UTC</b>).

<div class="panel panel-default">
<div class="panel-body">

<div class="card  m-3">

<div class="card-body">

<table style="margin-left: 1em;">
<tbody>
<tr><td>09:00</td><td>Welcome</td></tr>
<tr><td valign="top">09:05</td><td>The Clarity CEC3 Overview
 <table style="margin-left: 1em;">
        <tbody>
            <tr><td></td><td>Task 1 - Real impulse responses</td> <td>(Trevor Cox)</td></tr>
            <tr><td></td><td>Task 2 - Hearing aid microphones</td> <td>(Jon Barker)</td></tr>
            <tr><td></td><td>Task 3 - Real noise backgrounds</td> <td>(John Culling)</td></tr>
        </tbody>
    </table>
</td></tr>
<tr><td>09:40</td><td><span class="bold"><a href="#session1">Challenge Papers</a>  </span></td> <td>(Chair: Jennifer Firth)</td></tr>
<tr><td>11:10</td><td>Break</td></tr>
<tr><td>11:30</td><td><a href="#keynote">Invited Talk</a> </td><td>(Chair: Graham Naylor)</td></tr>
<tr><td>12:30</td><td>Clarity Future Directions </td> <td>(Chair: Simone Graetzer)</td></tr>
<tr><td>13:30</td><td>Close</td></tr>
</tbody>
</table>
</div>
</div>

Links to technical reports and videos for all talks are provided in the programme below.

<h1>Invited Talk</h1>

<div class="card m-3 mt-4">
  <a name="keynote"></a>

<div class="card-header">
<div class="row align-items-center">

<div class="col-sm-2">
<img src="./assets/images/avatar.png" alt="Speaker" class="float-left rounded-circle" style="width:60%; height:60%;" />
</div>

<div class="col-sm-2">
<h1 class="lead">Stefan Raufer / Peter Derleth <div class="text-muted">Sonova AG</div> </h1>
</div>

<div class="col-sm-6">

<h1> Deep learning-based denoising for hearing aids: Meeting the technological and audiological constraints of a wearable solution </h1>

<button class="btn btn-primary" style="color:white; margin: 10px; border-radius: 4px;" type="button" data-toggle="collapse" data-target="#collapseAbstractSonova" aria-expanded="false" aria-controls="collapseAbstractSonova">
Abstract
  </button>

</div>
</div>
</div>

<!---------------------------------------------------->
<div class="collapse" id="collapseAbstractSonova">

<div class="card-body">
<h1 class="card-title"> Deep learning-based denoising for hearing aids: Meeting the technological and audiological constraints of a wearable solution </h1>

<h2>Abstract</h2>

Improving speech understanding in noise is the top priority for people with hearing loss. While amplifying sound is sufficient to improve speech intelligibility in quiet environments, removing background noise is essential to improve speech clarity in noisy situations. In this keynote, we will review traditional noise cancelling approaches and discuss promises of DNN-based denoising approaches to improve speech intelligibility in noise. We will talk about the unique challenges of implementing DNN-based denoising solutions in wearables and constraints imposed by hearing aids. We will provide insights into Phonak’s recently launched Sphere DNN and highlight the co-development of the software architecture, hardware architecture, and training pipeline. Our speech enhancement DNN lives within the latency, power, and computational constraints of hearing aids and improves speech intelligibility in noise for people with hearing loss. We will furthermore discuss considerations for designing clinical studies and discuss study results of a recent study that demonstrated the clinical benefit of Phonak’s Sphere DNN solution. Lastly, we will talk about pitfalls in evaluating speech enhancement technologies for wearables, such as the neglect of latency considerations, open couplings, limitations of study setups, and using SNR as an outcome measure to demonstrate algorithm benefit

<!--<h2>Bio</h2>

TBA
-->
</div>
</div>
<!---------------------------------------------------->

</div>
</div>

<a name="session1"></a>

<h1>Challenge Papers</h1>

Challenge paper sessions will consist of oral presentations allowing 15-20 minutes per team and 5 minutes for Q&A.

<div class="card  m-3">
<div class="card-body">

<table>
<tbody>

<!--<div style="margin-bottom: 1em;">Session chair: <i>TBC</i></div>-->

<tr><td valign="top" style="margin-right: 5em; padding: 5px;"></td>
<td valign="top" style="margin-right: 5em; padding: 5px;">09:40-10:00</td><td style="padding: 5px;"><b> The JAIST System for Task 1 of The 3rd Clarity Enhancement Challenge  </b><br /> <span class="author">Huy Quoc Nguyen, Candy Olivia Mawalim, Masashi Unoki</span> <i>(Japan Advanced Institute of Science and Technology, Japan)</i></td>
</tr>

<tr><td valign="top" style="margin-right: 5em; padding: 5px;"></td>
<td valign="top" style="margin-right: 5em; padding: 5px;">10:00-10:20</td><td style="padding: 5px;"><b> MIMO-DPRNN-ConvTasNet for the 3rd Clarity Enhancement Challenge  </b><br /> <span class="author">Robert Sutherland, Stefan Goetze and Jon Barker</span> <i>(University of Sheffield, UK)</i></td>
</tr>

<tr><td valign="top" style="margin-right: 5em; padding: 5px;"></td>
<td valign="top" style="margin-right: 5em; padding: 5px;">10:20-10:45</td><td style="padding: 5px;"><b> End-to-End Multi-Channel Target Speech Enhancement System Based on Online SpatialNet  </b><br /> <span class="author">Yindi Yang, Ming Jiang, Zhihao Guo and Eric Miao</span> <i>(Elehear, Canada, China)</i></td>
</tr>

<tr><td valign="top" style="margin-right: 5em; padding: 5px;"></td>
<td valign="top" style="margin-right: 5em; padding: 5px;">10:45-11:10</td><td style="padding: 5px;"><b> Spatial-FullSubNet for Hearing Aid Processing  </b><br /> <span class="author">Xiang Hao, Jibin Wu</span> <i>(The Hong Kong Polytechnic University, China)</i></td>
</tr>

</tbody>
</table>

</div>
</div>

</div>
